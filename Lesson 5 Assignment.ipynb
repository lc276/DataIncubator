{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LiweiChen-L05-CategoryData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LiweiChen-L05-CategoryData.py\n",
    "\n",
    "# 1. import necessary packages\n",
    "from sklearn.preprocessing import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2. read in dataset and generate columns\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "auto_data = pd.read_csv(url,sep='\\s+', header=None)\n",
    "auto_data.columns = [\"mpg\", \"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model_year\", \"origin\", \"car_name\"]\n",
    "\n",
    "# 3. Normalize numeric values \n",
    "auto_data[['mpg', 'displacement', 'weight', 'acceleration']] = StandardScaler().fit_transform(auto_data[['mpg', 'displacement','weight', 'acceleration']])\n",
    "\n",
    "# 4. Bin mpg column into equal frequency percentiles\n",
    "percentiles = np.linspace(0, 100, 5)\n",
    "bounds = np.percentile(auto_data.loc[:,'mpg'], percentiles)\n",
    "auto_data.loc[:,'mpg'] = pd.cut(auto_data.loc[:,'mpg'], bounds, labels = ['1', '2', '3', '4'])\n",
    "\n",
    "# 5. Decode categorical data for origin column\n",
    "auto_data.loc[auto_data.loc[:, \"origin\"] == 1, \"origin\"] = \"USA\"\n",
    "auto_data.loc[auto_data.loc[:, \"origin\"] == 2, \"origin\"] = \"Canada\"\n",
    "auto_data.loc[auto_data.loc[:, \"origin\"] == 3, \"origin\"] = \"Mexico\"\n",
    "\n",
    "# 6. Impute missing categories for null values as unkonwn\n",
    "auto_data.loc[pd.isnull(auto_data.loc[:, \"origin\"]), \"origin\"] = \"Unknown\"\n",
    "\n",
    "# 7. Consolidate categorical data for origin into US vs non-US\n",
    "auto_data.loc[auto_data.loc[:, \"origin\"] == \"Canada\", \"origin\"] = \"non_US\"\n",
    "auto_data.loc[auto_data.loc[:, \"origin\"] == \"Mexico\", \"origin\"] = \"non_US\"\n",
    "\n",
    "#8 One-hot encode origin column  \n",
    "auto_data.loc[:, \"non_US\"] = (auto_data.loc[:, \"origin\"] == \"non_US\").astype(int)\n",
    "auto_data.loc[:, \"USA\"] = (auto_data.loc[:, \"origin\"] == \"USA\").astype(int)\n",
    "\n",
    "#9 Remove obsolete columns.\n",
    "auto_data = auto_data.drop(\"origin\", axis=1)\n",
    "\n",
    "#10 Present plots for categorical columns.\n",
    "auto_data.loc[:,\"USA\"].value_counts().plot(kind='bar')\n",
    "\n",
    "# Summary:\n",
    "# In this data, I only treated one category column: \"origin\".  I decoded 1, 2, 3 values as geographic locations of the cars\n",
    "# Any null values were imputed to be \"unknown\", though in this data there were no missing values.\n",
    "# For consolidation, I bucketed it into US vs non-US as the frequency of US is more than the other two combined.  \n",
    "# I one-hot encoded the resulting consolidated categories and plotted them to demonstrate that even with the consolidation, US has the highest frequency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
